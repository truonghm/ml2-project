{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Name Entity Recognition using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.family'] = 'Latin Modern Roman'\n",
    "\n",
    "DPI = 800\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Deep Transfer Learning\n",
    "\n",
    "The definition of deep transfer learning is as follows: given a learning task $T_t$ based on $D_t$, and a related but different learning task $T_s$ based on $D_s$, where $D_s$ and $D_t$ are the source and target domains, respectively, deep transfer learning aims to improve the learning of the target predictive function $f_t(\\cdot)$ in $T_t$ using the knowledge in $D_s$ and $T_s$, where $D_s \\neq D_t$ and/or $T_s \\neq T_t$. Among the various deep transfer learning techniques, network-based deep transfer learning is the most widely used. It is based on the assumption that the source and target domains share the same feature space but have different marginal probability distributions. Recently, pre-trained language models, such as GPT and BERT, with large amounts of unlabeled data and fine-tuning in downstream tasks have made a breakthrough in NLP domain (@fig-networktransfer).\n",
    "\n",
    "![The typical process of network-based deep transfer learning.](images/2023-09-18-23-06-37.png){#fig-networktransfer width=500}\n",
    "\n",
    "### BERT\n",
    "\n",
    "![Overall base architecture of BERT with twelve encoder blocks.](images/2023-09-19-00-30-13.png){#fig-bert width=400}\n",
    "\n",
    "Language models can be roughly categorized into N-gram language models and neural language models. While classical neural models, including Word2Vec, is still widely used today, BERT improves natural language pre-training by using mask-based objectives and a Transformer-based architecture (@fig-bert), which has successfully improved many state-of-the-art results for various natural language tasks. Since more powerful models like GPT3 are not open-source and not available to public, BERT can be regarded as one of the best pre-trained language models for downstream tasks.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "There are 2 dataset provided for the project, the training dataset and the testing dataset:\n",
    "\n",
    "| Dataset | No. of words/labels | No. of sentences/phrases |\n",
    "|---------|---------------------|--------------------------|\n",
    "| Training | 219552 | 23499 |\n",
    "| Testing | 55042 | 5946 |\n",
    "\n",
    "Each word (or token) is assigned a NER label that can be one of: \n",
    "\n",
    "- **B-MISC**: Beginning of a miscellaneous entity that doesn't fall under standard categories (like person, organization, or location).\n",
    "- **I-MISC**: Inside or continuation of a miscellaneous entity.\n",
    "- **B-PER**: Beginning of a person's name.\n",
    "- **I-PER**: Inside or continuation of a person's name. Used for multi-word names.\n",
    "- **O**: Outside of any named entity.\n",
    "- **B-LOC**: Beginning of a geographical location name.\n",
    "- **I-LOC**: Inside or continuation of a geographical location name. Used for multi-word locations.\n",
    "- **B-ORG**: Beginning of an organization name.\n",
    "- **I-ORG**: Inside or continuation of an organization name. Used for multi-word organizations.\n",
    "\n",
    "The **B-** prefix indicates the beginning of an entity, the **I-** prefix indicates that the word is inside an entity, and O indicates a word that is not part of a named entity.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Since the task of NER belongs to the group of multi-label classification problems, the following metrics are used to evaluate the performance of the models:\n",
    "\n",
    "- **Precision**: Precision measures the proportion of correctly identified named entities out of all entities the model identified. It is calculated using the formula:\n",
    "\n",
    "  $$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "  where `TP` is the number of True Positives (correctly identified entities), and `FP` is the number of False Positives (incorrectly identified entities).\n",
    "\n",
    "- **Recall**: Recall assesses the proportion of actual named entities that the model correctly identified. The formula for Recall is:\n",
    "\n",
    "  $$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "  where `FN` is the number of False Negatives (entities that were not identified).\n",
    "\n",
    "- **F1 Score**: The F1 Score is the harmonic mean of Precision and Recall, providing a balance between the two. It is particularly useful when the class distribution is uneven. F1 Score is calculated as:\n",
    "\n",
    "  $$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "- **Accuracy**: Accuracy measures the overall correctness of the model across all classifications, calculated by:\n",
    "\n",
    "  $$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "  where `TN` is the number of True Negatives (correctly identified non-entities). However, in NER, Accuracy is less informative due to the high number of true negatives (non-entity tokens), which can skew the metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The data was ingested and preprocessed in several steps:\n",
    "\n",
    "- **Verify and clean up the data**: training data and validation data are different, as one is separated by `;;;` and one is separated by a new line. Moreover, there are several samples that are mislabeled into \"O O\", when it should have been \"O\". In this case, we simply replace with the correct labels.\n",
    "- **Read the data into sentences (or phrases)**: In order to tokenize and train the data effectively, it needs to be transformed into sentences or phrases. To do this, we use the comma and the default separator (`;;;` or `\\n`) to group words that belong in the same sentence together.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "The data is tokenized using the `bert-base-uncased` tokenizer from HuggingFace, which is a platform for building, training, and deploying machine learning models, particularly focusing on pre-trained models like transformers. With this, we can transform the sentence into even futher broken-down pieces of sub-words. For example, the sentence *\"He said a proposal last month by EU Farm Commissioner Franz Fischler to ban sheep brains spleens and spinal cords from the human and animal food chains was a highly specific and precautionary move to protect human health '\"* can be tokenized into: \n",
    "\n",
    "```raw\n",
    "['he', 'said', 'a', 'proposal', 'last', 'month', 'by', 'eu', 'farm', 'commissioner', 'franz', 'fis', '##ch', '##ler', 'to', 'ban', 'sheep', 'brains', 'sp', '##leen', '##s', 'and', 'spinal', 'cords', 'from', 'the', 'human', 'and', 'animal', 'food', 'chains', 'was', 'a', 'highly', 'specific', 'and', 'pre', '##ca', '##ution', '##ary', 'move', 'to', 'protect', 'human', 'health', '.']\n",
    "```\n",
    "\n",
    "In this example, we can see that the word \"precautionary\" is split into 4 sub-words: `'pre', '##ca', '##ution', '##ary'`.\n",
    "\n",
    "### BERT Fine-tuning\n",
    "\n",
    "We use the same model `bert-base-uncased` for fine-tuning of the NER training dataset. The fine-tuning process can be summarized as follows:\n",
    "\n",
    "- Optimization Setup: We define the optimizer `AdamW` with the following parameters:\n",
    "\n",
    "  - Learning rate: $5e-5$. We also set up a learning rate scheduler to adjust the learning rate based on the number of warmup steps and total training steps.\n",
    "  - Adam $\\epsilon$: $1e-8$\n",
    "  - Weight decay: $0.0$\n",
    "\n",
    "- Training Loop: Iterate over the dataset for 10 epochs. During each batch, the following steps are executed:\n",
    "\n",
    "  - Forward pass: Compute the model's output and loss.\n",
    "  - Backward pass: Compute the gradient of the loss with respect to model parameters.\n",
    "  - Gradient Clipping: Clip gradients to a maximum norm to prevent exploding gradients.\n",
    "  - Optimization Step: Update model parameters using the optimizer.\n",
    "  - Learning Rate Scheduling: Update the learning rate based on the predefined schedule.\n",
    "  - Zero the gradients to prepare for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "After the fine-tuning process, we evaluate the model on the validation dataset. The following results are obtained in @tbl-general-results and @tbl-detail-results:\n",
    "\n",
    "\n",
    "| Metric    | Value  |\n",
    "|-----------|--------|\n",
    "| Accuracy  | 96.18% |\n",
    "| Precision | 78.03% |\n",
    "| Recall    | 82.56% |\n",
    "| F1        | 80.23% |\n",
    "\n",
    ": Accuracy, Precision, Recall and F1-score on validation dataset. {#tbl-general-results\"}\n",
    "\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| LOC   | 0.85      | 0.91   | 0.88     | 1837    |\n",
    "| MISC  | 0.87      | 0.83   | 0.85     | 922     |\n",
    "| ORG   | 0.59      | 0.67   | 0.63     | 1341    |\n",
    "| PER   | 0.82      | 0.85   | 0.83     | 1846    |\n",
    "\n",
    ": Detailed Metrics by Class. {#tbl-detail-results\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "\n",
    "The fine-tuning of BERT for Named Entity Recognition (NER) classification has yielded promising results, as evidenced by the metrics presented in the general and detailed results tables. Particularly, when looking at different classes, we can see that: \n",
    "\n",
    "- **Location (LOC)** entities achieved high precision and recall, leading to an F1 score of 0.88. This indicates the model's strong capability in identifying geographical entities, likely due to distinct contextual and syntactical patterns associated with such entities.\n",
    "\n",
    "- **Miscellaneous (MISC)** entities also showed strong performance with an F1 score of 0.85. The precision and recall balance suggests that the model is reasonably effective in identifying entities that do not fall into the more standard categories.\n",
    "\n",
    "- **Organizations (ORG)** is not very high at 0.63, as the lowest precision and F1 score among the categories. This could be due to the diverse nature of organization names and possible overlaps with other entity types, indicating a need for model improvement in this area.\n",
    "\n",
    "- **Person (PER)** names were well-recognized, with an F1 score of 0.83, reflecting the model's effectiveness in identifying individual names, possibly due to clear patterns and contextual cues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we have applied deep transfer learning and fine-tuning BERT for the task of Name Entity Recognition. The fine-tuned BERT model demonstrates strong potential in NER tasks, with particularly impressive results in identifying LOC and PER entities. The relatively lower performance on ORG entities suggests an area for further model refinement. Future work could explore more advanced techniques for handling ambiguous entities, additional contextual features, or more sophisticated post-processing rules to improve precision and recall across all entity types.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

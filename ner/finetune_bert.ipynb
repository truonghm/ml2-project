{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P2ZiwtUL-iGh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install transformers[torch] accelerate pandas numpy pyarrow seqeval datasets evaluate ipywidgets==7.7.1 jupyterlab-widgets==1.1.1\n",
    "\n",
    "# !curl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash -\n",
    "# !sudo apt install -y nodejs\n",
    "\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mypgFntP8zw2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING,\n",
    "    # AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nrrpw72T8zw4"
   },
   "outputs": [],
   "source": [
    "def load_train_data(data_path, gt_path):\n",
    "    with open(data_path) as f:\n",
    "        data = f.read().splitlines()\n",
    "\n",
    "        for i, w in enumerate(data):\n",
    "            if w == \";;;\":\n",
    "                data[i] = \"###\"\n",
    "            else:\n",
    "                data[i] = data[i].strip()\n",
    "\n",
    "        data = \"\".join(data).split(\"###\")\n",
    "        for i, t in enumerate(data):\n",
    "            data[i] = [x for x in t.split(\";;;\") if x != \"\"]\n",
    "\n",
    "        data = [x for x in data if x != [] and x != [\"\"]]\n",
    "\n",
    "    with open(gt_path, \"r\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "\n",
    "        for i, l in enumerate(labels):\n",
    "            if l == \"\":\n",
    "                labels[i] = \"###\"\n",
    "            else:\n",
    "                labels[i] = l.strip() + \";;;\"\n",
    "\n",
    "        labels = \"\".join(labels).split(\"###\")\n",
    "        for i, l in enumerate(labels):\n",
    "            labels[i] = [x for x in l.split(\";;;\") if x != \"\"]\n",
    "        labels = [x for x in labels if x != [] and x != [\"\"]]\n",
    "\n",
    "\n",
    "    text = []\n",
    "    gt = []\n",
    "\n",
    "    for d, l in zip(data, labels):\n",
    "        sentence = []\n",
    "        sentence_ner = []\n",
    "        for i, (w, ner) in enumerate(zip(d, l)):\n",
    "            if ner == 'O O':\n",
    "                sentence.append(w)\n",
    "                sentence_ner.append(\"O\")\n",
    "            elif \";\" not in ner:\n",
    "                sentence.append(w.lower())\n",
    "                sentence_ner.append(ner)\n",
    "\n",
    "        text.append(sentence)\n",
    "        gt.append(sentence_ner)\n",
    "\n",
    "    return text, gt\n",
    "\n",
    "def load_val_data(data_path, gt_path):\n",
    "    with open(data_path) as f:\n",
    "        data = f.read().splitlines()\n",
    "\n",
    "        for i, w in enumerate(data):\n",
    "            if w.strip() == \"\":\n",
    "                data[i] = \"###\"\n",
    "            else:\n",
    "                data[i] = data[i].strip() + \";;;\"\n",
    "\n",
    "        data = \"\".join(data).split(\"###\")\n",
    "        for i, t in enumerate(data):\n",
    "            data[i] = [x for x in t.split(\";;;\") if x != \"\"]\n",
    "\n",
    "        # data = [x for x in data if x != [] and x != [\"\"]]\n",
    "\n",
    "    with open(gt_path, \"r\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "\n",
    "        for i, l in enumerate(labels):\n",
    "            if l.strip() == \"\":\n",
    "                labels[i] = \"###\"\n",
    "            else:\n",
    "                labels[i] = l.strip() + \";;;\"\n",
    "\n",
    "        labels = \"\".join(labels).split(\"###\")\n",
    "        for i, l in enumerate(labels):\n",
    "            labels[i] = [x for x in l.split(\";;;\") if x != \"\"]\n",
    "        # labels = [x for x in labels if x != [] and x != [\"\"]]        \n",
    "    return data, labels\n",
    "    \n",
    "    # text = []\n",
    "    # gt = []\n",
    "\n",
    "    # for d, l in zip(data, labels):\n",
    "    #     sentence = []\n",
    "    #     sentence_ner = []\n",
    "    #     for i, (w, ner) in enumerate(zip(d, l)):\n",
    "    #         if ner == 'O O':\n",
    "    #             sentence.append(w)\n",
    "    #             sentence_ner.append(\"O\")\n",
    "    #         elif \";\" not in ner:\n",
    "    #             sentence.append(w.lower())\n",
    "    #             sentence_ner.append(ner)\n",
    "\n",
    "    #     text.append(sentence)\n",
    "    #     gt.append(sentence_ner)\n",
    "\n",
    "    # return text, gt\n",
    "\n",
    "def load_test_data(data_path):\n",
    "    with open(data_path) as f:\n",
    "        data = f.read().splitlines()\n",
    "\n",
    "        for i, w in enumerate(data):\n",
    "            if w.strip() == \"\":\n",
    "                data[i] = \"###\"\n",
    "            else:\n",
    "                data[i] = data[i].strip().lower() + \";;;\"\n",
    "\n",
    "        data = \"\".join(data).split(\"###\")\n",
    "        for i, t in enumerate(data):\n",
    "            data[i] = [x for x in t.split(\";;;\") if x != \"\"]\n",
    "\n",
    "        # data = [x for x in data if x != [] and x != [\"\"]]\n",
    "        return data\n",
    "\n",
    "def load_data(data_path, gt_path):\n",
    "\twith open(data_path) as f:\n",
    "\t\tdata = f.read().splitlines()\n",
    "\n",
    "\twith open(gt_path, \"r\") as f:\n",
    "\t\tlabels = f.read().splitlines()\n",
    "\n",
    "\tdf = pd.DataFrame({\"text\": data, \"label\": labels})\n",
    "\tdf = df[df[\"text\"] != \";;;\"]\n",
    "\tdf[\"text\"] = df[\"text\"].apply(lambda x: x.replace(\";;;\", \"\"))\n",
    "\tdf = df[~(df[\"label\"].str.strip()==\"\")]\n",
    "\tdf = df[~df[\"label\"].str.contains(\";\")]\n",
    "\n",
    "\tdf[\"label\"] = df[\"label\"].str.strip()\n",
    "\n",
    "\tdf[\"label\"] = np.where(df[\"label\"] == \"O O\", \"O\", df[\"label\"])\n",
    "\treturn df\n",
    "\n",
    "def rows_to_sentences_and_labels(df):\n",
    "    sentences = []\n",
    "    sentences_labels = []\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total = len(df)):\n",
    "        word, label = row['text'], row['label']\n",
    "        current_sentence.append(word.strip())\n",
    "        current_labels.append(label)\n",
    "        if word.strip() == '.':\n",
    "            sentences.append(current_sentence)\n",
    "            sentences_labels.append(current_labels)\n",
    "            current_sentence = []\n",
    "            current_labels = []\n",
    "\n",
    "    return sentences, sentences_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FpnwTq818zw5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5254b676ce9e4b8d812267c359f14e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/202386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83df8c734231418c8f8875e3645b51d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_data, train_labels = load_train_data(\"train_data.csv\", \"train_gt.csv\")\n",
    "# val_data, val_labels = load_val_data(\"valid_data.csv\", \"valid_gt.csv\")\n",
    "\n",
    "train = load_data(\"train_data.csv\", \"train_gt.csv\")\n",
    "valid = load_data(\"valid_data.csv\", \"valid_gt.csv\")\n",
    "\n",
    "train_data, train_labels = rows_to_sentences_and_labels(train)\n",
    "val_data, val_labels = rows_to_sentences_and_labels(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYzbuejp8zw7",
    "outputId": "a290231d-c8a4-45ce-b817-1e20fb8c7343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-MISC', 'B-PER', 'O', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC', 'I-PER', 'B-ORG'}\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set([x for y in train_labels for x in y])\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(lb) for lb in val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "def create_examples(data, labels, mode):\n",
    "    examples = []\n",
    "    guid_index = 1\n",
    "\n",
    "    for sent, lb in zip(data, labels):\n",
    "        input_example = InputExample(guid=f\"{mode}-{guid_index}\", words=sent, labels=lb)\n",
    "        examples.append(input_example)\n",
    "        guid_index += 1\n",
    "\n",
    "    return examples\n",
    "\n",
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    label_list,\n",
    "    max_seq_length,\n",
    "    tokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=\"[CLS]\",\n",
    "    cls_token_segment_id=1,\n",
    "    sep_token=\"[SEP]\",\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=-100,\n",
    "    sequence_a_segment_id=0,\n",
    "    mask_padding_with_zero=True,\n",
    "):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "            # bert-base-multilingual-cased sometimes output \"nothing ([]) when calling tokenize with just a space.\n",
    "            if len(word_tokens) > 0:\n",
    "                tokens.extend(word_tokens)\n",
    "                # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "                label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = tokenizer.num_special_tokens_to_add()\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "        else:\n",
    "            input_ids += [pad_token] * padding_length\n",
    "            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "            segment_ids += [pad_token_segment_id] * padding_length\n",
    "            label_ids += [pad_token_label_id] * padding_length\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)\n",
    "        )\n",
    "    return features\n",
    "\n",
    "def create_dataset(features):\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "LEARNING_RATE = 5e-5\n",
    "ADAM_EPSILON = 1e-8\n",
    "WEIGHT_DECAY = 0.0\n",
    "NUM_EPOCHS = 10\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8\n",
    "WARMUP_STEPS = 0\n",
    "SEED = 42\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model_type = \"bert\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label={str(i): label for i, label in enumerate(unique_labels)},\n",
    "    label2id={label: i for i, label in enumerate(unique_labels)},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = create_examples(train_data, train_labels, mode=\"train\")\n",
    "train_features = convert_examples_to_features(\n",
    "            train_examples,\n",
    "            unique_labels,\n",
    "            MAX_SEQ_LENGTH,\n",
    "            tokenizer,\n",
    "            cls_token_at_end=False,\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            cls_token_segment_id=0,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            sep_token_extra=False,\n",
    "            pad_on_left=False,\n",
    "            pad_token=tokenizer.pad_token_id,\n",
    "            pad_token_segment_id=tokenizer.pad_token_type_id,\n",
    "            pad_token_label_id=pad_token_label_id,\n",
    "        )\n",
    "\n",
    "train_dataset = create_dataset(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train(train_dataset, model, tokenizer, labels, pad_token_label_id, output_dir=\"./\", file_name=\"training_loss.txt\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    loss_file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "    t_total = len(train_dataloader) // GRADIENT_ACCUMULATION_STEPS * NUM_EPOCHS\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=ADAM_EPSILON)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=t_total)\n",
    "\n",
    "    set_seed()  # Set seed for reproducibility\n",
    "\n",
    "    global_step = 0\n",
    "    model.zero_grad()\n",
    "\n",
    "    train_iterator = trange(NUM_EPOCHS, desc=\"Epoch\")\n",
    "    for epoch in train_iterator:\n",
    "        epoch_loss = 0.0\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(model.device) for t in batch)\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            if \"token_type_ids\" in tokenizer.model_input_names:\n",
    "                inputs[\"token_type_ids\"] = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        # Calculate the average loss for the epoch and write it to a file\n",
    "        avg_epoch_loss = epoch_loss / len(epoch_iterator)\n",
    "        with open(loss_file_path, \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch + 1}: {avg_epoch_loss}\\n\")\n",
    "\n",
    "    return global_step, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad10f3495574652b1c5a14d5d592212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ff6f51c441452daad16bdfca18e4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af78013ed43a494c8133b1387ee0d398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b0f6586d9a4fe3bfa8441615d2bf4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91fe986edfe4b86a5d6f9d9822e774c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7daae541ef462c85233edc1a058256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb627198e294e16a9b1ff1c38932973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288f2b1571e24865a7d987b300492134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341347b336f144fb8f6ad1f61636e4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e76c5d6cf94e3098ba376198752900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417ffdbbd0cb4b888ff544adca485e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global_step, tr_loss = train(train_dataset, model, tokenizer, unique_labels, pad_token_label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9220, 0.0005266064956720548)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step, tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_dataset, model, tokenizer, labels, pad_token_label_id, device, model_type=\"bert\", prefix=\"\"):\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=1)\n",
    "\n",
    "    # Eval!\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            if model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "    for i in range(out_label_ids.shape[0]):\n",
    "        for j in range(out_label_ids.shape[1]):\n",
    "            if out_label_ids[i, j] != pad_token_label_id:\n",
    "                out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    results = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }\n",
    "\n",
    "    for key in sorted(results.keys()):\n",
    "        print(\"  %s = %s\", key, str(results[key]))\n",
    "\n",
    "    return results, preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6548d68a35422f9df3de91010cd4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1874 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  %s = %s f1 0.9399523851512211\n",
      "  %s = %s loss 0.09372719970730253\n",
      "  %s = %s precision 0.9375549692172384\n",
      "  %s = %s recall 0.9423620933521923\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "val_examples = create_examples(val_data, val_labels, mode=\"eval\")\n",
    "val_features = convert_examples_to_features(\n",
    "            val_examples,\n",
    "            unique_labels,\n",
    "            MAX_SEQ_LENGTH,\n",
    "            tokenizer,\n",
    "            cls_token_at_end=False,\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            cls_token_segment_id=0,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            sep_token_extra=False,\n",
    "            pad_on_left=False,\n",
    "            pad_token=tokenizer.pad_token_id,\n",
    "            pad_token_segment_id=tokenizer.pad_token_type_id,\n",
    "            pad_token_label_id=pad_token_label_id,\n",
    "        )\n",
    "\n",
    "val_dataset = create_dataset(val_features)\n",
    "result, preds_list = evaluate(val_dataset, model, tokenizer, unique_labels, pad_token_label_id, device, model_type, prefix=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He said a proposal last month by EU Farm Commissioner Franz Fischler to ban sheep brains spleens and spinal cords from the human and animal food chains was a highly specific and precautionary move to protect human health .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(train_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'said', 'a', 'proposal', 'last', 'month', 'by', 'eu', 'farm', 'commissioner', 'franz', 'fis', '##ch', '##ler', 'to', 'ban', 'sheep', 'brains', 'sp', '##leen', '##s', 'and', 'spinal', 'cords', 'from', 'the', 'human', 'and', 'animal', 'food', 'chains', 'was', 'a', 'highly', 'specific', 'and', 'pre', '##ca', '##ution', '##ary', 'move', 'to', 'protect', 'human', 'health', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\" \".join(train_data[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
    "id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_special_chars(input_string):\n",
    "    # Define the pattern to match everything except uppercase and lowercase letters\n",
    "    # This includes numbers, special characters, and whitespace\n",
    "    pattern = r'[^a-zA-Z]'\n",
    "    \n",
    "    # Use re.sub() to replace all occurrences of the pattern with an empty string\n",
    "    result = re.sub(pattern, '', input_string)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def contains_alphabet(s):\n",
    "    # This regular expression looks for any character between a-z or A-Z\n",
    "    pattern = re.compile('[a-zA-Z]')\n",
    "    # Search the string for any match to the pattern\n",
    "    if pattern.search(s):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def predict(sentence, model, max_length=MAX_SEQ_LENGTH):\n",
    "    sentence_check = []\n",
    "    for word in sentence:\n",
    "        if not contains_alphabet(word):\n",
    "            sentence_check.append(\"UNK\")\n",
    "        else:\n",
    "            sentence_check.append(remove_special_chars(word))\n",
    "\n",
    "    inputs = tokenizer(sentence_check,\n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        max_length=max_length,\n",
    "                       is_split_into_words=True,\n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "    ids = inputs[\"input_ids\"].to(device)\n",
    "    mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=ids, attention_mask=mask)\n",
    "        logits = outputs[0]\n",
    "    \n",
    "    active_logits = logits.view(-1, model.num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_predictions))\n",
    "    \n",
    "    word_level_predictions = []\n",
    "    for pair in wp_preds:\n",
    "      # print(pair)\n",
    "      \n",
    "      if (pair[0].startswith(\"##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "        continue\n",
    "      else:\n",
    "        # print(\"good\", pair[1])\n",
    "        word_level_predictions.append(pair[1])\n",
    "\n",
    "    if len(word_level_predictions) < len(sentence):\n",
    "        return word_level_predictions + predict(sentence[len(word_level_predictions):], model)\n",
    "    return word_level_predictions\n",
    "\n",
    "def save_to_file(mode, y_pred):\n",
    "    with open(f\"{mode}_gt.csv\", \"w\") as f:\n",
    "        f.write(\" \")\n",
    "        for sentence_labels in y_pred:\n",
    "            for label in sentence_labels:\n",
    "                f.write(label + \"\\n\")\n",
    "            f.write(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, val_labels = load_val_data(\"valid_data.csv\", \"valid_gt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046ffeda3e064b54835fb53522122027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_preds = [predict(sent, model) for sent in tqdm(val_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(val_labels) == len(eval_preds)\n",
    "\n",
    "for i, (p, t) in enumerate(zip(eval_preds, val_labels)):\n",
    "    assert isinstance(p, list)\n",
    "    assert isinstance(t, list)\n",
    "    if len(p) != len(t):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8023208302688568\n",
      "Accuracy: 0.9587333372597523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.85      0.91      0.88      1837\n",
      "        MISC       0.87      0.83      0.85       922\n",
      "         ORG       0.59      0.67      0.63      1341\n",
      "         PER       0.82      0.85      0.83      1846\n",
      "\n",
      "   micro avg       0.78      0.83      0.80      5946\n",
      "   macro avg       0.78      0.82      0.80      5946\n",
      "weighted avg       0.78      0.83      0.80      5946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "f1 = f1_score(val_labels, eval_preds)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "acc = accuracy_score(val_labels, eval_preds)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(classification_report(val_labels, eval_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file(\"valid_pred\", eval_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_gt.csv\n",
      "valid_pred_gt.csv\n",
      "processed 55042 tokens with 5946 phrases; found: 6291 phrases; correct: 4909.\n",
      "accuracy:  96.18%; \n",
      "precision:  78.03%;\n",
      "recall:  82.56%;\n",
      "F1:  80.23\n",
      "              LOC: prec:  85.18%; rec:  91.34%; FB1:  88.15  1970\n",
      "             MISC: prec:  86.75%; rec:  83.08%; FB1:  84.88  883\n",
      "              ORG: prec:  59.31%; rec:  67.49%; FB1:  63.13  1526\n",
      "              PER: prec:  81.59%; rec:  84.51%; FB1:  83.02  1912\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!python eval_stud.py valid_gt.csv valid_pred_gt.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_gt.csv\n",
      "train_fake_gt.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 219552 tokens with 23499 phrases; found: 23499 phrases; correct: 23499.\n",
      "accuracy: 100.00%; \n",
      "precision: 100.00%;\n",
      "recall: 100.00%;\n",
      "F1: 100.00\n",
      "              LOC: prec: 100.00%; rec: 100.00%; FB1: 100.00  7140\n",
      "             MISC: prec: 100.00%; rec: 100.00%; FB1: 100.00  3438\n",
      "              ORG: prec: 100.00%; rec: 100.00%; FB1: 100.00  6321\n",
      "              PER: prec: 100.00%; rec: 100.00%; FB1: 100.00  6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python eval_stud.py train_gt.csv train_fake_gt.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_test_data(\"test_data.csv\")\n",
    "# dummy_labels = []\n",
    "# for sent in test_data:\n",
    "#     dummy_labels.append([\"O\"] * len(sent))\n",
    "\n",
    "# # eval\n",
    "# test_examples = create_examples(test_data, dummy_labels, mode=\"test\")\n",
    "# test_features = convert_examples_to_features(\n",
    "#             test_examples,\n",
    "#             unique_labels,\n",
    "#             MAX_SEQ_LENGTH,\n",
    "#             tokenizer,\n",
    "#             cls_token_at_end=False,\n",
    "#             cls_token=tokenizer.cls_token,\n",
    "#             cls_token_segment_id=0,\n",
    "#             sep_token=tokenizer.sep_token,\n",
    "#             sep_token_extra=False,\n",
    "#             pad_on_left=False,\n",
    "#             pad_token=tokenizer.pad_token_id,\n",
    "#             pad_token_segment_id=tokenizer.pad_token_type_id,\n",
    "#             pad_token_label_id=pad_token_label_id,\n",
    "#         )\n",
    "\n",
    "# test_dataset = create_dataset(test_features)\n",
    "# _, test_preds_list = evaluate(test_dataset, model, tokenizer, unique_labels, pad_token_label_id, device, model_type, prefix=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = [predict(sent, model) for sent in tqdm(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file(\"test\", test_preds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
